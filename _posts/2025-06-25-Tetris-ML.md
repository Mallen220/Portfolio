---
layout: post
title: "Tetris Machine Learning"
summary: "Created a tetris machine learning model from scratch with custom weights and rewards."
author: MatthewAllen
date: "2025-06-25 0:00:00 +0530"
category: ["Python"]
thumbnail: /assets/img/projects/Tertris-ML.png
keywords: devlopr jekyll, how to use devlopr, devlopr, how to use devlopr-jekyll, devlopr-jekyll tutorial,best jekyll themes, multi languages and tags
usemathjax: true
permalink: /projects/Tetris-ML/
github_url: "https://github.com/Mallen220/Tetris-ML"
pagination:
  enabled: true
  collection: projects
---

### Tetris Machine Learning Project

**Project Description:**  
This project is a **Tetris implementation** enhanced with a **machine learning model** that learns to play the game autonomously. It features two distinct modes:

1. **Human Playable**: Classic Tetris with keyboard controls.
2. **ML Agent**: An AI that uses reinforcement learning (Deep Q-Network) to master Tetris by optimizing a custom reward function.
3. **Real-Time Visualizer**: A dashboard to monitor training metrics and agent performance.

---

### Key Features

1. **Dual Game Modes**:
   - **Human mode** with intuitive controls (arrow keys).
   - **AI mode** where a DQN agent learns optimal moves through self-play.
2. **Custom Reward Engine**:
   - **Engineered rewards/penalties** for strategic behaviors (e.g., clearing lines, minimizing height/holes, avoiding "wells").
   - Tuned heuristics like `flatness_bonus`, `setup_reward`, and `aggregate_height_penalty`.
3. **Training Dashboard**:
   - Real-time plotting of metrics (reward, lines cleared, penalties) during training.
   - Interactive controls to zoom, pan, and select metrics.
4. **Model Persistence**:
   - Save/load trained models (`DQN`) and high scores.
5. **Logging & Analysis**:
   - Comprehensive CSV logs for episodes, rewards, and game states.
   - Automated detection of the latest training data for visualization.

---

### Technical Implementation

#### Core Functionality:

- **Game Engine**:
  - Grid management, collision detection, piece rotation, and line-clearing logic.
  - Seven Tetromino shapes with color-coded visuals.
- **ML Environment** (Gymnasium-compatible):
  - **State Representation**: Flattened grid + piece info (position, rotation, next piece).
  - **Action Space**: 5 discrete actions (left/right/down/rotate/"plan").
  - **Reward Function**: Multi-component system balancing short/long-term strategy.
- **AI Agent** (Stable-Baselines3):
  - **DQN Algorithm**: Trains a neural network to predict Q-values for actions.
  - **Heuristic "Plan" Action**: Simulates future moves to choose optimal placement.
- **Visualizer**:
  - Renders live metrics using **Matplotlib** integrated with **Pygame**.
  - Dynamic UI with draggable timelines and metric toggles.

#### Technical Stack:

- **Languages**: Python
- **Libraries**:
  - `Pygame` (rendering/game logic), `Gymnasium` (RL environment), `Stable-Baselines3` (DQN).
  - `Pandas` (data handling), `Matplotlib` (visualization), `NumPy` (state processing).
- **Concurrency**: Multiprocessing for parallel training.

---

### Key Advantages

1. **Strategic Reward Design**:
   - Encourages behaviors beyond line-clearing (e.g., board "flatness", avoiding deep wells).
2. **Extensible Architecture**:
   - Decoupled game logic, ML environment, and visualization for easy upgrades.
3. **Real-Time Debugging**:
   - Visualizer helps diagnose reward function efficacy during training.
4. **Performance Optimization**:
   - Headless mode for faster training; rendering toggle for debugging.

---

### Setup Instructions

1. **Install Dependencies**:
   pip install pygame gymnasium stable-baselines3 pandas matplotlib
2. **Run Modes**:
   - **Human Play**: Execute `HumanPlayable.py`.
   - **Train AI**: Run `MLPlayable.py` (saves models to `tetris_dqn_model`).
   - **Visualizer**: Launch `tetris_visualizer.py` (auto-loads latest metrics).

---

### Special Notes

- **Reward Engineering**: The agent’s performance heavily depends on the reward function. Metrics like `holes`, `well_penalty`, and `flatness_bonus` were iteratively tuned.
- **Challenge**: The large state space (10x20 grid + piece states) makes convergence difficult. Techniques like frame-skipping and reward shaping were critical.
- **Visualizer Use Case**: Monitor long-term trends (e.g., if `aggregate_height_penalty` rises, the agent is stacking poorly).

---

### Summary

This project demonstrates **full-stack ML integration**—from game development and environment design to reinforcement learning and data visualization. Highlights include:

- A custom Gymnasium environment for Tetris.
- A DQN agent trained via tuned reward heuristics.
- An interactive dashboard to analyze training in real time.  
  **Skills Showcased**: Reinforcement learning, reward engineering, game development, and data visualization.
